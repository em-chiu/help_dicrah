{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gensim \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from  nltk.stem.snowball import FrenchStemmer, EnglishStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import pickle\n",
    "import dill as pickle\n",
    "import unidecode\n",
    "import sys\n",
    "from help_dicrah_functions import *\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# ML Tools\n",
    "from sklearn import preprocessing, linear_model, svm, model_selection, ensemble\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, make_scorer\n",
    "\n",
    "import time\n",
    "current_milli_time = lambda: int(round(time.time() * 1000))\n",
    "\n",
    "import os\n",
    "data_dir = '../data/'\n",
    "models_dir = '/Volumes/MIchi Hard /models/' #'../models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the following global variables to choose the type of processing and set output file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = True\n",
    "remove_sw = True\n",
    "\n",
    "if stem:\n",
    "    stem_str = '_with_stem'\n",
    "else:\n",
    "    stem_str = '_no_stem'\n",
    "\n",
    "    \n",
    "if remove_sw:\n",
    "    sw_str = '_sw_removed'\n",
    "else:\n",
    "    sw_str = '_no_sw_removed'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the sample to train w2v vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate time to clean the full sample (don't run, just for info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>id_tweet</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-04-28 08:45:41</td>\n",
       "      <td>1.255056e+18</td>\n",
       "      <td>Heureusement que le c√¥t√© Compassion et Pratiqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-04 06:18:22</td>\n",
       "      <td>1.257193e+18</td>\n",
       "      <td>#immobilier #COVID2019 #CoronavirusFrance #loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-04-27 13:33:48</td>\n",
       "      <td>1.254766e+18</td>\n",
       "      <td>RT @MathildePanot: üö® Parce qu‚Äôil existe une al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-04-30 11:53:41</td>\n",
       "      <td>1.255828e+18</td>\n",
       "      <td>RT @AntoineMaes: Le @RCLens qui gal√®re √† monte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-04-27 02:38:19</td>\n",
       "      <td>1.254601e+18</td>\n",
       "      <td>RT @holste_max: Un pays o√π il est plus facile ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0             Datetime      id_tweet  \\\n",
       "0          0  2020-04-28 08:45:41  1.255056e+18   \n",
       "1          1  2020-05-04 06:18:22  1.257193e+18   \n",
       "2          2  2020-04-27 13:33:48  1.254766e+18   \n",
       "3          3  2020-04-30 11:53:41  1.255828e+18   \n",
       "4          4  2020-04-27 02:38:19  1.254601e+18   \n",
       "\n",
       "                                               tweet  \n",
       "0  Heureusement que le c√¥t√© Compassion et Pratiqu...  \n",
       "1  #immobilier #COVID2019 #CoronavirusFrance #loc...  \n",
       "2  RT @MathildePanot: üö® Parce qu‚Äôil existe une al...  \n",
       "3  RT @AntoineMaes: Le @RCLens qui gal√®re √† monte...  \n",
       "4  RT @holste_max: Un pays o√π il est plus facile ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## look at data\n",
    "df = pd.read_csv('../data/Tweets_COVID19_France.csv')\n",
    "## goes back a folder \"../\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing: example of usage and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the full sample and export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, float found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36msimple_preprocess\u001b[0;34m(doc, deacc, min_len, max_len)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \"\"\"\n\u001b[1;32m    313\u001b[0m     tokens = [\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeacc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmin_len\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     ]\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text, lowercase, deacc, encoding, errors, to_lower, lower)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \"\"\"\n\u001b[1;32m    264\u001b[0m     \u001b[0mlowercase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlowercase\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mto_lower\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[0;34m(text, encoding, errors)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, float found"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parameters for w2v\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "#downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "# Prepare a list of tokens as input for w2v\n",
    "df['tokens'] = df['tweet'].apply( gensim.utils.simple_preprocess)\n",
    "\n",
    "# Takes ~8 mins to train with 300 features\n",
    "# model_1M = gensim.models.Word2Vec(documents, \n",
    "#                                   size=num_features, \n",
    "#                                   window=context, \n",
    "#                                   min_count=min_word_count,\n",
    "#                                   workers=4)\n",
    "# model_1M.train(documents, total_examples=len(documents), epochs=10)\n",
    "# with open(SAVED_MODEL, \"wb\") as f:\n",
    "#     pickle.dump(model_1M, f)\n",
    "\n",
    "        \n",
    "# w2v_1M = dict(zip(model_1M.wv.index2word, model_1M.wv.syn0))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit tfIdf vectorizer to weight w2v vectors (SLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: /Volumes/MIchi Hard /models/saved_w2vec_1M_vectorizer_with_stem_sw_removed_nf300_mvc1_cont10.pkl\n",
      "Training vectorizer...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'w2v_1M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w2v_1M' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "base_vec_name = 'saved_w2vec_1M_vectorizer'\n",
    "full_vec_name = base_vec_name+stem_str+sw_str\n",
    "\n",
    "\n",
    "SAVED_1M_VEC= models_dir+full_vec_name+'_nf'+str(num_features)+'_mvc'+str(min_word_count)\\\n",
    "            +'_cont'+str(context)+'.pkl'#+'_ds'+str(downsampling)\n",
    "\n",
    "print('Path: '+SAVED_1M_VEC)    \n",
    "    \n",
    "if not os.path.exists(SAVED_1M_VEC):\n",
    "\n",
    "        print(\"Training vectorizer...\")\n",
    "        # building 1M vectorizer\n",
    "        # Takes ~20 mins to train\n",
    "        my_vectorizer_1M = TfidfEmbeddingVectorizer(w2v_1M)\n",
    "        if remove_sw:\n",
    "            my_vectorizer_1M.fit(df.clean_tweets, stem=stem, rem_sw=True)\n",
    "        else:\n",
    "            my_vectorizer_1M.fit(df.clean_tweets, stem=stem, rem_sw=False)\n",
    "                \n",
    "        with open(SAVED_1M_VEC, \"wb\") as f:\n",
    "            pickle.dump(my_vectorizer_1M, f)\n",
    "\n",
    "else:\n",
    "        print(\"Loading vectorizer...\")\n",
    "        with open(SAVED_1M_VEC, \"rb\") as f:\n",
    "            my_vectorizer_1M = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    3057\n",
       "0.0     543\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>ID</th>\n",
       "      <th>Texte</th>\n",
       "      <th>Texte_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>998206424189751296</td>\n",
       "      <td>On m‚Äôavait sorti (pour de vrai) ¬´¬†on est tous ...</td>\n",
       "      <td>v  r  p u r  e  v r   e  u  e g   p u   p r ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>997896508384169984</td>\n",
       "      <td>VIVE L AMOUR ‚ù§VIVE LA PAIX\\n‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§üëèüëç\\nEN...</td>\n",
       "      <td>v v   u r  v v   p x  e r e  e  p e u p  u   p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>J'me rend compte √† quel point ya trop d'noir d...</td>\n",
       "      <td>e  r e  p   q u e  p   r p   r     e f   r p ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>998519485467414529</td>\n",
       "      <td>@KrypsKarmaKid Tu traites qui de salope la seu...</td>\n",
       "      <td>u  r  q u  e  p   e u  p e  e     e r   p u  e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>997799245464899584</td>\n",
       "      <td>Voil√† √† quoi tu dois me servir sale pute ! Ta ...</td>\n",
       "      <td>v   q u  u   e  e r v   p u   e u  u e  e  e  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                  ID  \\\n",
       "0    1.0  998206424189751296   \n",
       "1    1.0  997896508384169984   \n",
       "2    0.0                  31   \n",
       "3    0.0  998519485467414529   \n",
       "4    0.0  997799245464899584   \n",
       "\n",
       "                                               Texte  \\\n",
       "0  On m‚Äôavait sorti (pour de vrai) ¬´¬†on est tous ...   \n",
       "1  VIVE L AMOUR ‚ù§VIVE LA PAIX\\n‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§üëèüëç\\nEN...   \n",
       "2  J'me rend compte √† quel point ya trop d'noir d...   \n",
       "3  @KrypsKarmaKid Tu traites qui de salope la seu...   \n",
       "4  Voil√† √† quoi tu dois me servir sale pute ! Ta ...   \n",
       "\n",
       "                                         Texte_clean  \n",
       "0    v  r  p u r  e  v r   e  u  e g   p u   p r ...  \n",
       "1  v v   u r  v v   p x  e r e  e  p e u p  u   p...  \n",
       "2   e  r e  p   q u e  p   r p   r     e f   r p ...  \n",
       "3  u  r  q u  e  p   e u  p e  e     e r   p u  e...  \n",
       "4  v   q u  u   e  e r v   p u   e u  u e  e  e  ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = data_dir+'tweets_labeled_4k_cleaned.csv'\n",
    "\n",
    "tw_data = pd.read_csv(data_path)#.drop('id', axis=1).rename(columns={'label': 'Label', 'tweet': 'Texte'})\n",
    "\n",
    "# Fix labels\n",
    "labels = {'n':1, 'N':1, 'h':0, 'H':0, 'ENG':-1, 's':0, \n",
    "          'S':0, 'SH':0, 'PUB':-1, 'sh':0, 'f':-1, 'nn':1}\n",
    "\n",
    "tw_data['Label'].replace(labels, inplace=True)\n",
    "tw_data.drop(tw_data[tw_data.Label == -1].index, inplace=True) #removing ads etc.\n",
    "\n",
    "# Clean text. Youc an choose whether to remove stopwords and stem.\n",
    "# Note: use the same pre-processing used for the dataset on which w2v has been trained !\n",
    "# In particular, here I don't remove stopwords, \n",
    "# since they could change substantially the meaning of a phrase\n",
    "\n",
    "if not remove_sw:\n",
    "    tw_data['Texte_clean'] = tw_data.Texte.apply(lambda x:tweet_cleaner(x, my_dict, stem=stem))\n",
    "else:\n",
    "    tw_data['Texte_clean'] = tw_data.Texte.apply(lambda x:' '.join(remove_stopwords(\n",
    "                        tokenize(\n",
    "                        tweet_cleaner(\n",
    "                            x,\n",
    "                            my_dict,\n",
    "                            stem=stem)),\n",
    "                    generate_stopwords())))\n",
    "\n",
    "\n",
    "\n",
    "tw_data = tw_data.drop_duplicates(subset='Texte_clean')\n",
    "tw_data = tw_data.dropna()\n",
    "\n",
    "\n",
    "tw_data = tw_data.reset_index(drop=True)\n",
    "tw_data['Label'].value_counts()\n",
    "tw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_1M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_1M' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(len(model_1M.wv.index2word), \"mots ont √©t√© vectoris√©s\")\n",
    "print(\"Chaque mot est repr√©sent√© selon\", model_1M.wv.syn0[0].shape[0], \"dimensions\")\n",
    "\n",
    "words_list = []\n",
    "for tweet in list(tw_data['Texte_clean']):\n",
    "    for w in list(tweet.split()):\n",
    "        words_list.append(w)\n",
    "\n",
    "words_data = pd.DataFrame(words_list).drop_duplicates()\n",
    "\n",
    "count = 0\n",
    "for w in list(words_data[0]):\n",
    "    if w not in model_1M.wv.index2word:\n",
    "        count +=1\n",
    "\n",
    "print(count, \"mots sur\", len(words_data), \"sont dans les tweets clean et pas dans le dict w2v\")\n",
    "print(len(words_data))\n",
    "print(len(words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'as_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'as_matrix'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# lexical features\n",
    "tw_data = add_lexical_features(tw_data)\n",
    "lex_cols = ['nbr_characters','nbr_words', 'nbr_ats', 'nbr_hashtags', 'nbr_urls',\n",
    "            'nbr_letters','nbr_caps', 'nbr_fancy' ]\n",
    "lex_features = tw_data.as_matrix(columns=lex_cols)\n",
    "\n",
    "# w2v features, weighted\n",
    "features_1M_data = my_vectorizer_1M.transform(tw_data.Texte_clean)\n",
    "\n",
    "# w2v features, no weight\n",
    "my_vectorizer_1M_noweight = MeanEmbeddingVectorizer(w2v_1M)\n",
    "features_1M_not_weighted = my_vectorizer_1M_noweight.transform(tw_data.Texte_clean)\n",
    "\n",
    "\n",
    "\n",
    "# tfidf features. We impose a feature vector of 90% the size of the dataset, \n",
    "# so that n_features < n_data. This can be changed\n",
    "\n",
    "features_tfidf, words_freq, vocab = get_tfidf_frequencies(tw_data.Texte_clean,\n",
    "                                                            stem=True,\n",
    "                                                            remove_stopwords=False,\n",
    "                                                            ngram_range=(1,3),\n",
    "                                                          n_features = round((tw_data.shape[0]/10)*9)\n",
    "                                                         )\n",
    "\n",
    "\n",
    "# converting the 4K labeled data into one vector, \n",
    "# in order to keep only the closests to re-run w2v embedding\n",
    "labels_vector = features_1M_data.mean(axis=0)\n",
    "\n",
    "\n",
    "# Concatenation\n",
    "M_1M = np.concatenate([features_1M_data, lex_features],axis=1)\n",
    "print('w2v features dimension: %s' %str(M_1M.shape))  \n",
    "\n",
    "M_tfidf = np.concatenate([features_tfidf, lex_features],axis=1)\n",
    "print('Tf-Idf features dimension: %s' %str(M_tfidf.shape))\n",
    "\n",
    "\n",
    "M_not_weighted = np.concatenate([features_1M_not_weighted, lex_features],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select  a subset of the 1M tweets that is closer to the labels vector to re-train w2v embedding (SLOW )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: /Volumes/MIchi Hard /models/saved_w2vec_selected_with_stem_sw_removed_nf300_mvc1_cont10.pkl\n",
      "Transforming 1 Milion features with w2v...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'my_vectorizer_1M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_vectorizer_1M' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "base_model_name_1 = 'saved_w2vec_selected'\n",
    "full_model_name_1 = base_model_name_1+stem_str+sw_str\n",
    "\n",
    "SAVED_MODEL_1 = models_dir+full_model_name_1+'_nf'+str(num_features)+'_mvc'+str(min_word_count)\\\n",
    "            +'_cont'+str(context)+'.pkl'#+'_ds'+str(downsampling)\n",
    "\n",
    "print('Path: '+SAVED_MODEL_1 )\n",
    "    \n",
    "if not os.path.exists(SAVED_MODEL_1):\n",
    "    \n",
    "    t = current_milli_time()\n",
    "    print(\"Transforming 1 Milion features with w2v...\") # This takes ~2min\n",
    "    features_1M_1M = my_vectorizer_1M.transform(df.clean_tweets)\n",
    "    print(\"it takes about\",(current_milli_time()-t)/60000,\"minutes\")\n",
    "    \n",
    "    print(\"Re-fitting model...\")\n",
    "    t = current_milli_time()\n",
    "     \n",
    "    # measuring distances between 1M tweets baseline and the average vector\n",
    "    # of the 4K labelled tweets\n",
    "\n",
    "    norm_labels = np.linalg.norm(labels_vector)\n",
    "    norm_features = np.linalg.norm(features_1M_1M, axis=1)\n",
    "    dot = labels_vector.dot(features_1M_1M.T)\n",
    "    df['similarity'] = dot/(norm_labels*norm_features)\n",
    "    df['len'] = [len(t.split()) for t in df.clean_tweets]\n",
    "    \n",
    "    # designing a subset of the 1M tweets that is closer to the labels vector,\n",
    "    # to re-train w2v embedding\n",
    "    seuil_len = 5\n",
    "    seuil_sim = 0.5\n",
    "    df_selected = df.sort_values(by='similarity', ascending=False).drop(df[df.len<seuil_len].index)\n",
    "    df_selected = df_selected.drop(df_selected[df_selected.similarity<seuil_sim].index)\n",
    "\n",
    "    # generate documents list for training w2v\n",
    "    documents_selected = [tweet.split() for tweet in list(df_selected['clean_tweets'])]\n",
    "    \n",
    "    \n",
    "    \n",
    "    model_selected = gensim.models.Word2Vec(documents_selected, \n",
    "                                      size=num_features, \n",
    "                                      window=context, \n",
    "                                      min_count=min_word_count,\n",
    "                                      workers=4)\n",
    "    model_selected.train(documents_selected, total_examples=len(documents_selected), epochs=10)\n",
    "    print(\"it takes about\",(current_milli_time()-t)/60000,\"minutes\")\n",
    "    with open(SAVED_MODEL_1, \"wb\") as f:\n",
    "        pickle.dump(model_selected, f)\n",
    "else:\n",
    "    print(\"Loading model...\")\n",
    "    with open(SAVED_MODEL_1, \"rb\") as f:\n",
    "        model_selected = pickle.load(f)\n",
    "        \n",
    "w2v_selected = dict(zip(model_selected.wv.index2word, model_selected.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: /Volumes/MIchi Hard /models/saved_w2vec_selected_vectorizer_with_stem_sw_removed_nf300_mvc1_cont10.pkl\n",
      "Fitting vectorizer...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'w2v_selected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w2v_selected' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "base_vec_name_1 = 'saved_w2vec_selected_vectorizer'\n",
    "full_vec_name_1 = base_vec_name_1+stem_str+sw_str\n",
    "\n",
    "\n",
    "SAVED_VEC_1= models_dir+full_vec_name_1+'_nf'+str(num_features)+'_mvc'+str(min_word_count)\\\n",
    "            +'_cont'+str(context)+'.pkl'#+'_ds'+str(downsampling)\n",
    "\n",
    "\n",
    "print(\"Path: \"+ SAVED_VEC_1)\n",
    "        \n",
    "if not os.path.exists(SAVED_VEC_1):\n",
    "    \n",
    "    print(\"Fitting vectorizer...\")\n",
    "    # Takes ~ 10mins\n",
    "    t = current_milli_time()\n",
    "\n",
    "    my_vectorizer_selected = TfidfEmbeddingVectorizer(w2v_selected)\n",
    "    if remove_sw:\n",
    "        my_vectorizer_selected.fit(df_selected['clean_tweets'], stem=stem, rem_sw=True)\n",
    "    else:\n",
    "        my_vectorizer_selected.fit(df_selected['clean_tweets'], stem=stem, rem_sw=False)\n",
    "\n",
    "\n",
    "    print(\"it takes about\",(current_milli_time()-t)/60000,\"minutes\")\n",
    "    \n",
    "    with open(SAVED_VEC_1, \"wb\") as f:\n",
    "        pickle.dump(my_vectorizer_selected, f)\n",
    "\n",
    "else:\n",
    "    print(\"Loading vectorizer...\")\n",
    "    with open(SAVED_VEC_1, \"rb\") as f:\n",
    "          my_vectorizer_selected = pickle.load(f)\n",
    "\n",
    "\n",
    "            \n",
    "features_selected = my_vectorizer_selected.transform(tw_data.Texte_clean)\n",
    "\n",
    "M_selected = np.concatenate([features_selected,lex_features],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'M_1M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mc/1k6qg5js1x31mqzdq17v2yym0000gn/T/ipykernel_16368/936749268.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m X_train_1M, X_test_1M, y_train_1M, y_test_1M = train_test_split(M_1M,\n\u001b[0m\u001b[1;32m      4\u001b[0m                                                                 \u001b[0my\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                 \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'M_1M' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "y = tw_data['Label']\n",
    "\n",
    "X_train_1M, X_test_1M, y_train_1M, y_test_1M = train_test_split(M_1M,\n",
    "                                                                y ,\n",
    "                                                                random_state=42, \n",
    "                                                                test_size=0.15)\n",
    "\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(M_tfidf, y ,\n",
    "                                                                            random_state=42, \n",
    "                                                                            test_size=0.15)\n",
    "\n",
    "X_train_not_weighted, X_test_not_weighted,\\\n",
    "y_train_not_weighted, y_test_not_weighted = train_test_split(M_not_weighted, \n",
    "                                                     y ,\n",
    "                                                     random_state=42, \n",
    "                                                     test_size=0.15)\n",
    "\n",
    "\n",
    "X_train_selected, X_test_selected,\\\n",
    "y_train_selected, y_test_selected = train_test_split(M_selected, \n",
    "                                                     y ,\n",
    "                                                     random_state=42, \n",
    "                                                     test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = linear_model.LogisticRegression(C=1., class_weight='balanced',penalty='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_1M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_1M' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_model_w = lr_model\n",
    "lr_model_w.fit(X_train_1M, y_train_1M)\n",
    "y_preds_w = lr_model_w.predict(X_test_1M)\n",
    "\n",
    "print(classification_report( y_test_1M, y_preds_w ))\n",
    "cm_w = print_cm(y_test_1M, y_preds_w , ['H','N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-weighted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_not_weighted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_not_weighted' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_model_uw = lr_model\n",
    "\n",
    "lr_model_uw.fit(X_train_not_weighted, y_train_not_weighted)\n",
    "y_preds_uw = lr_model_uw.predict(X_test_not_weighted)\n",
    "\n",
    "print(classification_report( y_test_not_weighted, y_preds_uw ))\n",
    "cm_uw = print_cm(y_test_not_weighted, y_preds_uw , ['H','N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selected weighted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_selected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_selected' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_model_sw = lr_model\n",
    "\n",
    "lr_model_sw.fit(X_train_selected, y_train_selected)\n",
    "y_preds_sw = lr_model_sw.predict(X_test_selected)\n",
    "\n",
    "print(classification_report( y_test_selected, y_preds_sw ))\n",
    "cm_sw = print_cm(y_test_selected, y_preds_sw , ['H','N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-Idf only features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.65      0.44      0.52        73\n",
      "        1.0       0.92      0.96      0.94       473\n",
      "\n",
      "avg / total       0.88      0.89      0.88       546\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_model= ensemble.RandomForestClassifier(criterion='gini')\n",
    "\n",
    "rf_model.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_preds_rf = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "print(classification_report( y_test_tfidf, y_preds_rf ))\n",
    "cm_rf = print_cm(y_test_tfidf, y_preds_rf , ['H','N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling+Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0.0: 545, 1.0: 545})\n",
      "(981,)\n",
      "(109,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "rus = RandomUnderSampler(return_indices=True)\n",
    "X_res, y_res, idx_res = rus.fit_sample(M_selected, y)\n",
    "print('Resampled dataset shape {}'.format(Counter(y_res)))\n",
    "\n",
    "X_res_train, X_res_test, y_res_train, y_res_test = train_test_split(X_res, \n",
    "                                                     y_res ,\n",
    "                                                     random_state=42, \n",
    "                                                     test_size=0.1)\n",
    "\n",
    "print(y_res_train.shape)\n",
    "print(y_res_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.73      0.79      0.76        56\n",
      "        1.0       0.76      0.70      0.73        53\n",
      "\n",
      "avg / total       0.74      0.74      0.74       109\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "lr_model_us = lr_model\n",
    "\n",
    "lr_model_us.fit(X_res_train, y_res_train)\n",
    "y_preds_us = lr_model_us.predict(X_res_test)\n",
    "\n",
    "print(classification_report( y_res_test, y_preds_us ))\n",
    "cm_us = print_cm(y_res_test, y_preds_us , ['H','N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "Weighted features seem to do better. So, the higher evaluation time is worth. \n",
    "Also, w2v with tf-idf weights does significantly better than Tf-Idf alone.\n",
    "On the other hand, it is not completely clear if using selected features gives better performance.\n",
    "In the following, we will optimize using w2v features with tf-idf weights, and compare features obtained from selected and full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid parameter search\n",
    "####  As a metric, we maximise recall for class \"H\" (it is better to predict an \"H\" which is actually an \"N\" than the contrary). The function custom_recall does this and will be used as evaluation metric to choose the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_recall(y_true, y_pred): \n",
    "    ''' Computes recall only on labels 0 '''\n",
    "    rec = recall_score(y_true, y_pred, labels=[0], pos_label=0, average='binary')\n",
    "    return rec #target_accuracy\n",
    "\n",
    "my_scorer = make_scorer(custom_recall)\n",
    "\n",
    "\n",
    "\n",
    "def my_pipeline(my_clf, x_train, x_test , y_train, y_test, scorer, parameters):\n",
    "\n",
    "    pipeline = Pipeline([('clf', my_clf)])\n",
    "\n",
    "    nn = x_train.shape[0]\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1,verbose=1, scoring=scorer)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    print ('Best score: %0.3f' % grid_search.best_score_)\n",
    "    print ('Best parameters set:')\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "    predictions = grid_search.predict(x_test)\n",
    "    print (classification_report(y_test, predictions))\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression - weighted features from full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_init = linear_model.LogisticRegression(C=1., class_weight='balanced',penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "parameters_lr = {\n",
    "        #'clf__penalty': ('l1', 'l2'),\n",
    "        'clf__C': np.linspace(1, 10, 2),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "class_LR_w = my_pipeline(lr_init, \n",
    "                       X_train_1M, X_test_1M, y_train_1M, y_test_1M,\n",
    "                       my_scorer , parameters_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm_w = print_cm(y_test_1M, class_LR_w.predict(X_test_1M) , ['H','N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression - weighted features from selected dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "parameters_lr = {\n",
    "        #'clf__penalty': ('l1', 'l2'),\n",
    "        'clf__C': np.linspace(1, 10, 2),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "class_LR_selected = my_pipeline(lr_init, \n",
    "                       X_train_selected, X_test_selected, y_train_selected, y_test_selected,\n",
    "                       my_scorer , parameters_lr)\n",
    "\n",
    "print_cm(y_test_selected, class_LR_selected.predict(X_test_selected) , ['H','N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm_selected = print_cm(y_test_selected, class_LR_w.predict(X_test_selected) , ['H','N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression - unweighted features from full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "parameters_lr = {\n",
    "        'clf__penalty': ('l1', 'l2'),\n",
    "        'clf__C': np.linspace(1, 10, 2),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "class_LR_uw = my_pipeline(linear_model.LogisticRegression(C=1., \n",
    "                                                       class_weight='balanced'\n",
    "                                                       ), \n",
    "                       X_train_not_weighted, X_test_not_weighted,\n",
    "                          y_train_not_weighted, y_test_not_weighted,\n",
    "                       my_scorer , parameters_lr)\n",
    "\n",
    "print_cm(y_test_not_weighted, class_LR_uw.predict(X_test_not_weighted) , ['H','N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm_uw = print_cm(y_test_not_weighted, class_LR_uw.predict(X_test_not_weighted) , ['H','N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest + TFIDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_rf = {\n",
    "        'clf__criterion': ('gini', 'entropy'),\n",
    "        'clf__max_depth': (None, 10, 100),\n",
    "        'clf__min_samples_leaf':(1,5,10)\n",
    "    }\n",
    "\n",
    "class_RF = my_pipeline(ensemble.RandomForestClassifier(n_jobs=-1, random_state=321),\n",
    "                       X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf,\n",
    "                       my_scorer, parameters_rf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm_RF = print_cm(y_test_tfidf, class_RF.predict(X_test_tfidf) , ['H','N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "parameters_lr = {\n",
    "       'clf__penalty': ('l1', 'l2'),\n",
    "        'clf__C': np.linspace(1, 10, 2),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "class_LR_us = my_pipeline(linear_model.LogisticRegression(C=1.), \n",
    "                       X_res_train, X_res_test, y_res_train, y_res_test,\n",
    "                       my_scorer , parameters_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_cm(y_res_test, class_LR_us.predict(X_res_test) , ['H','N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Very promising but results can fluctuate a lot depending on the sampling. Must add k-fold cross validation !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other trials (no luck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_model= svm.SVC(C=1, kernel='rbf', gamma=0.1)\n",
    "\n",
    "svm_model.fit(X_train_1M, y_train_1M)\n",
    "y_preds = svm_model.predict(X_test_1M)\n",
    "\n",
    "print(classification_report( y_test_1M, y_preds ))\n",
    "print_cm(y_test_1M, y_preds , ['H','N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_model= ensemble.RandomForestClassifier(criterion='entropy', n_estimators=10)\n",
    "\n",
    "rf_model.fit(X_train_1M, y_train_1M)\n",
    "y_preds = rf_model.predict(X_test_1M)\n",
    "\n",
    "print(classification_report( y_test_1M, y_preds ))\n",
    "print_cm(y_test_1M, y_preds , ['H','N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "parameters_svm = {\n",
    "        'clf__gamma': (0.0001, 0.001, 0.01),\n",
    "        'clf__C': (100, 150, 500),\n",
    "        #'clf__kernel':('rbf', 'linear','sigmoid' )\n",
    "    }\n",
    "\n",
    "class_SVM = my_pipeline(svm.SVC(C=100, kernel='rbf'),\n",
    "                        X_train_1M, X_test_1M, y_train_1M, y_test_1M,\n",
    "                       my_scorer , parameters_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_cm(y_test_1M, class_SVM.predict(X_test_1M) , ['H','N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5, metric='chebyshev')\n",
    "\n",
    "knn.fit(X_train_1M, y_train_1M)\n",
    "y_pred_1M = knn.predict(X_test_1M)\n",
    "\n",
    "print_cm(y_test_1M, y_pred_1M , ['H','N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=10, metric='minkowski')\n",
    "\n",
    "knn.fit(X_res_train, y_res_train)\n",
    "y_pred_us = knn.predict(X_res_test)\n",
    "\n",
    "print_cm(y_res_test, y_pred_us , ['H','N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our conclusion: the best model is Logistic REgression with l1 reg. and C=1 .\n",
    "It seem to work (slightly) better with  w2v trained on the full sample rather than on the selected sample (why? Because the selected sample it's trained with less data? ). However, the difference is tiny and more investigation seems important (e.g trying cross-validation).\n",
    "\n",
    "\n",
    "If we use a Tf-Idf vectorizer, we get that the best classifier is a Random Forest with gini criterion. This does considerably worse that w2v+log reg if we consider the False positive rate (predicted =N but true=H ), and better if we consider the false negative (predicted=H but true=N). \n",
    "Since it's better to minimize false positive rate, we conclude that w2v works better.  \n",
    "\n",
    "Finally, using undersampling techniques seems to improve the performace, in particular on the false positive rate, even if the size of the dataset is highly reduced. Given the small amount of data available, however, this risks to worsen the classifier's performance when applied to previously unseen data, and also the performance can fluctuate considerably when changing subsampling. However, this result is interesting to keep in mind if one has a larger dataset.\n",
    "\n",
    "#### In general, to better comment performance we need to cross-validate ! Otherwise the results fluctuate too much (this is natural given the nature of the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVED_MODEL = models_dir+\"saved_lr_1M.pkl\"\n",
    "with open(SAVED_MODEL, \"wb\") as f:\n",
    "    pickle.dump(class_LR, f)\n",
    "\n",
    "SAVED_MODEL = models_dir+\"saved_lr_selected.pkl\"\n",
    "with open(SAVED_MODEL, \"wb\") as f:\n",
    "    pickle.dump(class_LR_selected, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "<img src=\"https://datascientest.fr/train/assets/logo_datascientest.png\", style=\"height:150px\">\n",
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<h1 style = \"text-align:center\" > Practice Python </h1> \n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "\n",
    "* pour ajouter une cellule utiliser esc + a\n",
    "* pour supprimer une cellule utiliser esc + x (faire attention)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
